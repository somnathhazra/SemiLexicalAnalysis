{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Validity of Soduku Board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import Inference as inf\n",
    "def check_validity(line):\n",
    "    data = np.array([int(j) for j in line]).reshape((9,9))\n",
    "    inf.conflict_pairs={}\n",
    "    inf.conflict_set=set()\n",
    "    return inf.check_valid_board(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Non Valid Soduku Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "file = open(\"notvalid.txt\", \"w\")\n",
    "\n",
    "n = 81\n",
    "for i in range(10000):\n",
    "    line = ''.join([\"{}\".format(random.randint(1, 9)) for num in range(0, n)])\n",
    "    valid = check_validity(line)\n",
    "    #print(valid)\n",
    "    if not valid:\n",
    "        line = line + ',0\\n'\n",
    "        file.write(line)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data for Combined Valid and Non Valid Data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(): \n",
    "\n",
    "    data = pd.read_csv('Dataset/Combined.csv')\n",
    "\n",
    "    feat_raw = data['Puzzle']\n",
    "    label_raw = data['Valid']\n",
    "\n",
    "    feat = []\n",
    "    label = []\n",
    "\n",
    "    for i in feat_raw:\n",
    "    \n",
    "        x = np.array([int(j) for j in i]).reshape((9,9,1))\n",
    "        feat.append(x)\n",
    "    \n",
    "    feat = np.array(feat)\n",
    "    feat = feat/9\n",
    "    feat -= .5    \n",
    "    \n",
    "    '''for i in label_raw:\n",
    "    \n",
    "        x = np.array(label_raw[i])\n",
    "        label.append(x)   '''\n",
    "    \n",
    "    label = np.array(label_raw)\n",
    "    \n",
    "    del(feat_raw)\n",
    "    del(label_raw)    \n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(feat, label, test_size=0.5, random_state=42,shuffle=True)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = get_data()\n",
    "#print(f'{x_train[0]}----{y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model for checking if Soduko board configuarion is Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, BatchNormalization, Dense, Flatten, Reshape\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same', input_shape=(9,9,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, kernel_size=(1,1), activation='relu', padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 9, 9, 64)          640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 9, 9, 128)         8320      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                663616    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 710,081\n",
      "Trainable params: 709,825\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9902/9902 [==============================] - 15s 1ms/step - loss: 0.4329 - accuracy: 0.8850\n",
      "Epoch 2/2\n",
      "9902/9902 [==============================] - 13s 1ms/step - loss: 0.2568 - accuracy: 0.9966 1s - loss:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27f3ef46f98>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.adam(lr=.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Confusion Matrix for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [ True]\n",
      " [False]\n",
      " ...\n",
      " [ True]\n",
      " [False]\n",
      " [False]]\n",
      "[0 1 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "y_pred =(y_pred>0.5)\n",
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4944    7]\n",
      " [   0 4951]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SodokuValidator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(feat_raw):\n",
    "    \n",
    "    feat = np.array([int(j) for j in feat_raw]).reshape((1,9,9,1))\n",
    "    #feat.append(x)\n",
    "    \n",
    "    feat = np.array(feat)\n",
    "    feat = feat/9\n",
    "    feat -= .5   \n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert random ambiguities in valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.633241]]\n"
     ]
    }
   ],
   "source": [
    "test = '641874532239139687837625419163957248982416753475382196328741965519263874746598321'\n",
    "data = process(test)\n",
    "#print(data)\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Inserts ambiguities at random positions\n",
    "def create_ambiguous():\n",
    "    data = pd.read_csv('Dataset/Valid.csv')\n",
    "    feat = []\n",
    "    random.seed(32)\n",
    "    feat_raw = data['Puzzle']\n",
    "    file1 = open(\"semilexicalras.txt\", \"w\")\n",
    "    file2 = open(\"semilexicalraslabel.txt\", \"w\")\n",
    "    for i in feat_raw:\n",
    "        x = np.array([int(j) for j in i]).reshape((9,9))\n",
    "        y = np.copy(x).tolist()\n",
    "        #print(x)\n",
    "        rowrange = random.randint(0,8)+1\n",
    "        colrange = random.randint(0,8)+1\n",
    "        #print(rowrange)\n",
    "        #print(colrange)\n",
    "        newdict = {}\n",
    "        for i in range(rowrange):\n",
    "            for j in range(colrange):\n",
    "                if x[i][j] == 4:\n",
    "                    x[i][j] = 0\n",
    "                    y[i][j] = '9_4'\n",
    "                    newdict[(i,j)] = {9:50,4:50}\n",
    "                if x[i][j] == 9:\n",
    "                    x[i][j] = 0\n",
    "                    y[i][j] = '4_9'\n",
    "                    newdict[(i,j)] = {9:50,4:50}\n",
    "                if x[i][j] == 3:\n",
    "                    x[i][j] = 0\n",
    "                    y[i][j] = '5_3'\n",
    "                    newdict[(i,j)] = {3:50,5:50}\n",
    "                if x[i][j] == 5:\n",
    "                    x[i][j] = 0\n",
    "                    y[i][j] = '3_5'\n",
    "                    newdict[(i,j)] = {3:50,5:50}\n",
    "        str1= ''\n",
    "        str2= ''\n",
    "        for i in range(9):\n",
    "            for j in range(9):\n",
    "                str2=str2+str(y[i][j])\n",
    "                str1=str1+str(x[i][j])\n",
    "        line1 = str1 + ',1\\n'\n",
    "        file1.write(line1)\n",
    "        line2 = str2 + '\\n'\n",
    "        file2.write(line2)\n",
    "        #print(newdict)\n",
    "        #print(x.tolist())\n",
    "        #print(y)\n",
    "        #print(str1)\n",
    "        #print(str2)\n",
    "    file1.close()\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ambiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run end to end CNN model for semi-lexical tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1030]\n"
     ]
    }
   ],
   "source": [
    "fileSL = open(\"semilexicalras.txt\", \"r\")\n",
    "model = keras.models.load_model('SodokuValidator.h5')\n",
    "sl_test = []\n",
    "for line in fileSL:\n",
    "    sl_test.append(line.split(',')[0])\n",
    "feat = []\n",
    "for i in sl_test:\n",
    "    \n",
    "    x = np.array([int(j) for j in i]).reshape((9,9,1))\n",
    "    feat.append(x)\n",
    "    \n",
    "feat = np.array(feat)\n",
    "feat = feat/9\n",
    "feat -= .5\n",
    "\n",
    "y_pred=model.predict(feat)\n",
    "y_pred =(y_pred>0.5)\n",
    "print(sum(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Semi-Lexical Framework for board with semi-lexical tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "82\n",
      "['3_5', '1', '4_9', '6', '2', '8', '4', '7', '3', '8', '7', '5_3', '4_9', '4', '1', '6', '5', '2', '6', '4', '2', '3', '7', '5', '9', '1', '8', '9', '3', '4', '7', '1', '6', '2', '8', '5', '2', '5', '6', '4', '8', '3', '7', '9', '1', '1', '8', '7', '2', '5', '9', '3', '4', '6', '4', '2', '8', '5', '6', '7', '1', '3', '9', '7', '9', '5', '1', '3', '2', '8', '6', '4', '3', '6', '1', '8', '9', '4', '5', '2', '7', '\\n']\n",
      "[[0 1 0 6 2 8 4 7 3]\n",
      " [8 7 0 0 4 1 6 5 2]\n",
      " [6 4 2 3 7 5 9 1 8]\n",
      " [9 3 4 7 1 6 2 8 5]\n",
      " [2 5 6 4 8 3 7 9 1]\n",
      " [1 8 7 2 5 9 3 4 6]\n",
      " [4 2 8 5 6 7 1 3 9]\n",
      " [7 9 5 1 3 2 8 6 4]\n",
      " [3 6 1 8 9 4 5 2 7]]\n"
     ]
    }
   ],
   "source": [
    "fileSL = open(\"semilexicalras.txt\", \"r\")\n",
    "fileSLLaber = open(\"semilexicalraslabel.txt\", \"r\")\n",
    "sl_test = []\n",
    "sl_label = []\n",
    "\n",
    "for line in fileSL:\n",
    "    sl_test.append(line.split(',')[0])\n",
    "for line in fileSLLaber:\n",
    "    boardlist1 = []\n",
    "    boardlist2 = []\n",
    "    for element in line:\n",
    "        boardlist1.append(element)\n",
    "    x = 0\n",
    "    while (x<len(boardlist1)):\n",
    "        if x < len(boardlist1)-1 and boardlist1[x+1] == '_':\n",
    "            strx = boardlist1[x]+boardlist1[x+1]+boardlist1[x+2]\n",
    "            boardlist2.append(strx)\n",
    "            x  = x+3\n",
    "        else:\n",
    "            boardlist2.append(boardlist1[x])\n",
    "            x=x+1\n",
    "        \n",
    "    sl_label.append(boardlist2)\n",
    "print(len(boardlist1))\n",
    "feat = []\n",
    "for i in sl_test:  \n",
    "    x = np.array([int(j) for j in i]).reshape((9,9))\n",
    "    feat.append(x)\n",
    "print(len(sl_label[0]))\n",
    "print(sl_label[0])\n",
    "print(feat[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1 4 6 2 8 4 7 3]\n",
      " [8 7 5 4 4 1 6 5 2]\n",
      " [6 4 2 3 7 5 9 1 8]\n",
      " [9 3 4 7 1 6 2 8 5]\n",
      " [2 5 6 4 8 3 7 9 1]\n",
      " [1 8 7 2 5 9 3 4 6]\n",
      " [4 2 8 5 6 7 1 3 9]\n",
      " [7 9 5 1 3 2 8 6 4]\n",
      " [3 6 1 8 9 4 5 2 7]]\n"
     ]
    }
   ],
   "source": [
    "semilexicalcell_predlist = []\n",
    "count = 0\n",
    "while count < len(feat):\n",
    "    semilexicalpreddict = {}\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if feat[count][i][j] == 0:\n",
    "                mapdict = {}\n",
    "                predlist = sl_label[count][((i*9)+j)].split('_')\n",
    "                mapdict[predlist[0]] = 50\n",
    "                mapdict[predlist[1]] = 50\n",
    "                semilexicalpreddict[(i,j)] = mapdict\n",
    "    count = count + 1\n",
    "    semilexicalcell_predlist.append(semilexicalpreddict)\n",
    "print(feat[0])\n",
    "#print(semilexicalcell_predlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9804\n"
     ]
    }
   ],
   "source": [
    "import Inference as inf\n",
    "number_count = 0\n",
    "for i in range(len(feat)):\n",
    "    val = inf.sd.call_solve_sudoku(feat[i],semilexicalcell_predlist[i])\n",
    "    if(isinstance(val,np.ndarray)):\n",
    "        number_count = number_count+1\n",
    "print(number_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
